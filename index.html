<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Reproducible computation at scale in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Will Landau" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link href="index_files/remark-css/default.css" rel="stylesheet" />
    <link href="index_files/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reproducible computation at scale in R
### Will Landau

---


&lt;style&gt;
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 10px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
.remark-slide-content .nocopyright:after {
content: "";
}
.small {
  font-size: 85%;
}
&lt;/style&gt;





## Demanding computation in R

* Deep learning: `keras`, `tensorflow`, `torch`
* Machine learning: `tidymodels`
* Bayesian data analysis: JAGS, Stan, NIMBLE, `greta`
* PK/PD: `nlmixr`, `mrgsolve`
* Clinical trial simulation: `rpact`, `Mediana`
* Statistical genomics
* Social network analysis
* Permutation tests
* Database queries: `DBI`
* ETL on large data

---

## Overlooked realities of long computation

![](./images/realities.png)

---

## Workflows have interconnected steps.

![](./images/workflow.png)

---

## If you change code or data...

![](./images/change.png)

---

## ...the downstream steps are no longer valid.

![](./images/downstream.png)

---

## Dilemma: short runtimes or reproducible results?

![](./images/decisions.png)

---

## Let a pipeline tool figure out what to rerun.

![](./images/pipeline_graph.png)

* Save time while ensuring computational reproducibility.
* Automatic parallel/distributed computing based on the directed acyclic graph.

---

## Pipeline tools

&lt;center&gt;
&lt;img src="./images/infographic.png" height = "125px"&gt;
&lt;/center&gt;

* Existing pipeline tools: https://github.com/pditommaso/awesome-pipeline
* Most are language-agnostic or designed for Python or the shell.

## {targets}

* Fundamentally designed for R.
* Supports a clean, modular, function-oriented programming style.
* Abstracts files as R objects and automatically manages data.
* Surpasses the permanent limitations of its predecessor, [`drake`](https://github.com/ropensci/drake): &lt;https://books.ropensci.org/targets/drake.html&gt;.
* Continuation of the ideas from `remake` by Rich FitzJohn: &lt;https://github.com/richfitz/remake&gt;.

---

## {targets} embraces functions.

&gt; Nearly everything that happens in R results from a function call. Therefore, basic programming centers on creating and refining functions.
&gt;
&gt; [John Chambers (2008)](https://www.springer.com/gp/book/9780387759357)

* Functions are fundamental building blocks in most computer languages.
* Advantages of functions:
    * Clearly define relationships among inputs and outputs.
    * Help `targets` decide which tasks to run and which ones to skip.
    * Break down complicated ideas into manageable pieces.
    * Custom shorthand to make code easier to read.
* Common uses of R functions in data science:
    1. Preprocess a dataset.
    2. Analyze a dataset.
    3. Summarize an analysis.

---

## Example: customer churn

* Deep learning with Keras.
* IBM Cognos Analytics Telco customer churn dataset.
* Find the model that best predicts which customers will cancel their telecom subscriptions.
* Example comes from Matt Dancho's 2018 RStudio AI Blog post: https://blogs.rstudio.com/ai/posts/2018-01-11-keras-customer-churn/.

---

## Project files

* Many variations on this theme, only `_targets.R` is strictly required.


```r
_targets.R
R/
└── functions.R
data/
└── customer_churn.csv
```

---

## functions.R file


```r
split_data &lt;- function(file) {
  read_csv(file, col_types = cols()) %&gt;%
    initial_split(prop = 0.7)
}

prepare_recipe &lt;- function(data) {
  # ...
}

test_model &lt;- function(data, recipe, act1) {
  # ...
}
```

---

## _targets.R file


```r
library(targets)
source("R/functions.R")
tar_option_set(packages = c("keras", "tidyverse", "rsample", "recipes", "yardstick"))
list(
  tar_target(file, "data/churn.csv", format = "file"),
  tar_target(data, split_data(file)),
  tar_target(recipe, prepare_recipe(data)),
  tar_target(model1, test_model(data, recipe, act1 = "relu")),
  tar_target(model2, test_model(data, recipe, act1 = "sigmoid")),
  tar_target(model3, test_model(data, recipe, act1 = "linear"))
)
```

---

## Check the graph.


```r
tar_visnetwork(targets_only = TRUE)
```

![](./images/keras-before.png)

---

## Run the pipeline.


```r
tar_make()
#&gt; • start target file
#&gt; • built target file
#&gt; • start target data
#&gt; • built target data
#&gt; • start target recipe
#&gt; • built target recipe
#&gt; • start target model1
#&gt; • built target model1
#&gt; • start target model2
#&gt; • built target model2
#&gt; • start target model3
#&gt; • built target model3
#&gt; • end pipeline
```

---

## Check the results.


```r
tar_read(model1)
#&gt; # A tibble: 1 x 2
#&gt;   accuracy act1 
#&gt;      &lt;dbl&gt; &lt;chr&gt;
#&gt; 1    0.794 relu 
tar_read(model3)
#&gt; # A tibble: 1 x 2
#&gt;   accuracy act1  
#&gt;      &lt;dbl&gt; &lt;chr&gt; 
#&gt; 1    0.798 linear
```

---

## Change a target.


```r
library(targets)
source("R/functions.R")
tar_option_set(packages = c("keras", "tidyverse", "rsample", "recipes", "yardstick"))
list(
  tar_target(file, "data/churn.csv", format = "file"),
  tar_target(data, split_data(file)),
  tar_target(recipe, prepare_recipe(data)),
  tar_target(model1, test_model(data, recipe, act1 = "relu")),
  tar_target(model2, test_model(data, recipe, act1 = "sigmoid")),
* tar_target(model3, test_model(data, recipe, act1 = "softmax"))
)
```

---

## Only model3 will rerun next time.


```r
tar_visnetwork(targets_only = TRUE)
```

![](./images/keras-command.png)

---

## The other models are skipped to save time.


```r
tar_make()
#&gt; ✓ skip target file
#&gt; ✓ skip target data
#&gt; ✓ skip target recipe
#&gt; ✓ skip target model1
#&gt; ✓ skip target model2
#&gt; • start target model3
#&gt; • built target model3
#&gt; • end pipeline
```

---

## Change function code.


```r
define_model &lt;- function(churn_recipe, act1) {
  input_shape &lt;- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  out &lt;- keras_model_sequential() %&gt;%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %&gt;%
*   layer_dropout(rate = 0.2) # previously 0.1
  # ...
}
```

---

## {targets} understand dependencies among functions.


```r
tar_visnetwork(targets_only = FALSE)
```

&lt;image src="./images/keras-function.png" height = "400px"&gt;

---

## Run the models but skip the data.


```r
tar_make()
#&gt; ✓ skip target file
#&gt; ✓ skip target data
#&gt; ✓ skip target recipe
#&gt; • start target model1
#&gt; • built target model1
#&gt; • start target model2
#&gt; • built target model2
#&gt; • start target model3
#&gt; • built target model3
#&gt; • end pipeline
```

---

## Tangible evidence of reproducibility


```r
tar_make()
#&gt; ✓ skip target file
#&gt; ✓ skip target data
#&gt; ✓ skip target recipe
#&gt; ✓ skip target model1
#&gt; ✓ skip target model2
#&gt; ✓ skip target model3
#&gt; ✓ skip pipeline
```

---

## Extending {targets}

![](./images/targetopia.png)

---

## Target factories

* A target factory is a reusable function that creates target objects.


```r
target_factory &lt;- function(file) {
  list(
    tar_target_raw("file", file, format = "file", deployment = "main"),
    tar_target_raw("data", quote(read_data(file)), format = "fst_tbl", deployment = "main"),
    tar_target_raw("model", quote(run_model(data)), format = "qs")
  )
}
```

---

## Target factories simplify pipeline construction.


```r
# _targets.R
library(targets)
library(yourExamplePackage)
list(
  target_factory("data.csv")
)
```


```r
# R console
tar_manifest(fields = command)
#&gt; # A tibble: 3 x 2
#&gt;   name  command          
#&gt;   &lt;chr&gt; &lt;chr&gt;            
#&gt; 1 file  "\"data.csv\""   
#&gt; 2 data  "read_data(file)"           
#&gt; 3 model "run_model(data)"
```

---

## Example: {stantargets}

&lt;center&gt;
&lt;image src="./images/stantargets.png" height = "300px"&gt;
&lt;/center&gt;

* Easy pipeline construction for Stan statistical models.
* Uses R packages [`cmdstanr`](https://mc-stan.org/cmdstanr/) and [`posterior`](https://mc-stan.org/posterior/).

---

## About Stan

* Probabilistic programming language ([Carpenter et al. 2017](https://www.jstatsoft.org/article/view/v076i01)).
* Markov chain Monte Carlo (MCMC) with HMC and NUTS.
    * Often more efficient than Gibbs sampling.
    * Flexible specification of posterior distributions.
    * Indifferent to conjugacy.
* Variational inference (ADVI)
* Penalized MLE (L-BFGS)

---

## Example Stan model


```c
// model.stan
data {
  int &lt;lower = 1&gt; n;
  vector[n] x;
  vector[n] y;
}
parameters {
  real alpha;
  vector[n] beta;
}
model {
  y ~ normal(alpha + x .* beta, 1);
  alpha ~ normal(0, 1);
  beta ~ normal(0, 1);
}
```

---

## Interval-based validation study

* For thousands of independent replications:
    * Simulate data from the prior predictive distribution.
    * Fit the model to the simulated data using MCMC.
    * Calculate x% posterior intervals for each of `alpha` and `beta`.
* For each of `alpha` and `beta`, roughly x% of the posterior intervals should cover the corresponding parameter draws from the joint prior.
* Simulation-based calibration extends this idea further ([Cook et al. 2006](https://www.jstor.org/stable/27594203); [Talts et al. 2020](https://arxiv.org/abs/1804.06788)).

---

## Function for prior predictive simulations


```r
simulate_data &lt;- function(n = 10L) {
  alpha &lt;- rnorm(n = 1, mean = 0, sd = 1)
  beta &lt;- rnorm(n = n, mean = 0, sd = 1)
  x &lt;- seq(from = -1, to = 1, length.out = n)
  y &lt;- rnorm(n, alpha + x * beta, 1)
  list(
    n = n,
    x = x,
    y = y,
    # Hold on to prior draws:
*   .join_data = list(alpha = alpha, beta = beta)
  )
}
```

---

## _targets.R file


```r
library(targets)
library(stantargets)
source("R/functions.R")
tar_option_set(packages = "tidyverse")
*options(clustermq.scheduler = "multicore")
list(
  tar_stan_mcmc_rep_summary(
    name = model, stan_files = "model.stan", data = simulate_data(),
    batches = 80, reps = 25, variables = c("alpha", "beta"),
    summaries = list(
      ~posterior::quantile2(.x, probs = c(0.025, 0.975)),
      rhat = posterior::rhat
    ),
  ),
  tar_target(
    coverage,
    model %&gt;%
      group_by(variable) %&gt;%
      summarize(rate = mean(q2.5 &lt; .join_data &amp; .join_data &lt; q97.5))
  )
)
```

---

## Multiple targets

![](./images/stan-graph.png)

---

## Run MCMC batches in parallel


```r
&gt; tar_make_clustermq(workers = 4) # 4 local cores
#&gt; • start target model_batch
#&gt; • built target model_batch
#&gt; • start branch model_data_b0b9380a
#&gt; • start branch model_data_ffcdb73c
#&gt; • start branch model_data_b968a03a
#&gt; • start branch model_data_f8763cb2
#&gt; • built branch model_data_ffcdb73c
#&gt; • start branch model_data_0bfdabdc
#&gt; • built branch model_data_f8763cb2
#&gt; • start branch model_data_d888319c
#&gt; ...
```

* High-performance computing with `targets`: &lt;https://books.ropensci.org/targets/hpc.html&gt;
* Distributed computing available, not just multicore: SLURM, SGE, TORQUE, PBS, LSF 
* Persistent workers with the [`clustermq`](https://mschubert.github.io/clustermq/) package: `tar_make_clustermq()`
* Transient workers with the [`future`](https://future.futureverse.org/) package: `tar_make_future()`

---

## Simulation results


```r
tar_read(model)
#&gt; # A tibble: 22,000 x 8
#&gt;    variable   q2.5 q97.5  rhat .join_data .rep     .file      .name
#&gt;    &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;
#&gt;  1 alpha     0.575  1.92  1.00      0.768 14556f0a model.stan model
#&gt;  2 beta[1]  -1.34   1.50  1.00      0.163 14556f0a model.stan model
#&gt;  3 beta[2]  -0.620  2.51  1.00      1.14  14556f0a model.stan model
#&gt;  4 beta[3]  -1.81   1.67  1.00      1.03  14556f0a model.stan model
#&gt;  5 beta[4]  -2.22   1.51  1.00      0.638 14556f0a model.stan model
#&gt;  6 beta[5]  -1.97   1.99  1.00     -1.03  14556f0a model.stan model
#&gt;  7 beta[6]  -1.99   1.90  1.00     -0.348 14556f0a model.stan model
#&gt;  8 beta[7]  -1.28   2.52  1.00     -0.570 14556f0a model.stan model
#&gt;  9 beta[8]  -1.80   1.63  1.00      0.832 14556f0a model.stan model
#&gt; 10 beta[9]  -1.84   1.33  1.00      0.551 14556f0a model.stan model
# … with 21,990 more rows
```

---

## Coverage is nominal (95%)


```r
tar_read(coverage)
#&gt; # A tibble: 11 x 2
#&gt;    variable  rate
#&gt;    &lt;chr&gt;    &lt;dbl&gt;
#&gt;  1 alpha    0.950
#&gt;  2 beta[1]  0.948
#&gt;  3 beta[10] 0.959
#&gt;  4 beta[2]  0.954
#&gt;  5 beta[3]  0.962
#&gt;  6 beta[4]  0.949
#&gt;  7 beta[5]  0.952
#&gt;  8 beta[6]  0.960
#&gt;  9 beta[7]  0.952
#&gt; 10 beta[8]  0.952
#&gt; 11 beta[9]  0.944
```

---

## Resources

* `targets`: &lt;https://docs.ropensci.org/targets/&gt;
* `stantargets`:  &lt;https://docs.ropensci.org/stantargets/&gt;
* Stan: &lt;https://mc-stan.org/&gt;

## References

.small[
* Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A probabilistic programming language. Journal of Statistical Software 76(1). [10.18637/jss.v076.i01](https://www.jstatsoft.org/article/view/v076i01).
* Samantha R. Cook, Andrew Gelman, and Donald B. Rubin. 2006. "Validation of Software for Bayesian Models Using Posterior Quantiles." Journal of Computational and Graphical Statistics 15 (3): 675–92. &lt;http://www.jstor.org/stable/27594203&gt;.
* John M. Chambers. 2008. &lt;u&gt;Software for Data Analysis: Programming with R&lt;/u&gt;. Chapter 3. Springer, 978-1441926128, &lt;https://www.springer.com/gp/book/9780387759357&gt;.
* Matt Dancho. "Deep Learning With Keras To Predict Customer Churn." RStudio AI Blog, 2018-01-11. &lt;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/&gt;.
* Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. "Validating Bayesian Inference Algorithms with Simulation-Based Calibration." &lt;http://arxiv.org/abs/1804.06788&gt;.
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
